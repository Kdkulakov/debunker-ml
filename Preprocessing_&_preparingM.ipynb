{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df5ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\RV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Preparing processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c6a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "from urllib.parse import quote_plus as quote\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "import pymongo\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import RequestsHttpConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8a4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import io\n",
    "from io import StringIO\n",
    "import string\n",
    "from collections import Counter\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natasha\n",
    "import razdel\n",
    "from razdel import tokenize\n",
    "from razdel import sentenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce335510",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'mongodb://{user}:{pw}@{hosts}/?replicaSet={rs}&authSource={auth_src}'.format(user=quote('user_news'),\n",
    "       pw=quote('wfn20fg42n)FEen0wfe'),\n",
    "       hosts=','.join(['rc1b-04te123feq45i8fg.mdb.yandexcloud.net:27018']),\n",
    "       rs='rs01',\n",
    "       auth_src='db_news')\n",
    "dbs = pymongo.MongoClient(url,tlsCAFile='allCAs.pem')['db_news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a9531f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2880/4169742063.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnews_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mnews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__comment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             )\n\u001b[1;32m-> 1188\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__send_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[1;34m(self, operation)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m             response = client._run_operation(\n\u001b[0m\u001b[0;32m   1053\u001b[0m                 \u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__address\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_run_operation\u001b[1;34m(self, operation, unpack_res, address)\u001b[0m\n\u001b[0;32m   1265\u001b[0m             )\n\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         return self._retryable_read(\n\u001b[0m\u001b[0;32m   1268\u001b[0m             \u001b[0m_cmd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             \u001b[0moperation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_retryable_read\u001b[1;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[0;32m   1369\u001b[0m                         \u001b[1;32massert\u001b[0m \u001b[0mlast_error\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mlast_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1371\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_pref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mServerSelectionTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mretrying\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_cmd\u001b[1;34m(session, server, sock_info, read_preference)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_cmd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_preference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m             return server.run_operation(\n\u001b[0m\u001b[0;32m   1264\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_preference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_listeners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munpack_res\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\server.py\u001b[0m in \u001b[0;36mrun_operation\u001b[1;34m(self, sock_info, operation, read_preference, listeners, unpack_res)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# Unpack and check for command errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(self, request_id)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreceive_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_message_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raise_if_not_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munacknowledged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(self, request_id)\u001b[0m\n\u001b[0;32m    792\u001b[0m         \"\"\"\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mreceive_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_message_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\network.py\u001b[0m in \u001b[0;36mreceive_message\u001b[1;34m(sock_info, request_id, max_message_size)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_receive_data_on_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressor_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_receive_data_on_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pymongo\\network.py\u001b[0m in \u001b[0;36m_receive_data_on_socket\u001b[1;34m(sock_info, length, deadline)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mchunk_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbytes_read\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: B014\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_errno_from_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url = 'mongodb://{user}:{pw}@{hosts}/?replicaSet={rs}&authSource={auth_src}'.format(\n",
    "user=quote('user_news'),\n",
    "    pw=quote('wfn20fg42n)FEen0wfe'),\n",
    "    hosts=','.join(['rc1b-04te123feq45i8fg.mdb.yandexcloud.net:27018']),\n",
    "    rs='rs01',\n",
    "    auth_src='db_news')\n",
    "dbs = pymongo.MongoClient(url,tlsCAFile='allCAs.pem')['db_news']\n",
    "news_cursor = dbs.test_collection.find()\n",
    "\n",
    "news = []\n",
    "for col in dbs.news_test.find({}):\n",
    "    news.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"news_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26870cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train.head()}\\n{train.info()}')\n",
    "train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1900259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['url'].value_counts()\\\n",
    "                 .sort_index(ascending=False)\\\n",
    "                 .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['snippet'].value_counts()\\\n",
    "                 .sort_index(ascending=False)\\\n",
    "                 .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.groupby('body_new')['snippet'].value_counts()\n",
    "train_1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78775dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Пропорции NaN значений в body_new values: {train.body_new.isna().sum() / len(train)}\")\n",
    "print(f\"Пропорции NaN значений в snippet values: {train.snippet.isna().sum() / len(train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lowercase_text'] = train['snippet'].str.lower()\n",
    "print(train['lowercase_text'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем на наличие url и хэштегов, потом их удаляем, как и пробелы\n",
    "train[train.snippet.str.contains(\"https.\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lowercase_text']=train['lowercase_text'].str.replace('   ', ' ')\n",
    "train['lowercase_text']=train['lowercase_text'].str.replace('     ', ' ')\n",
    "train['lowercase_text']=train['lowercase_text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\n",
    "train['lowercase_text']=train['lowercase_text'].str.replace('  ', ' ')\n",
    "train['lowercase_text']=train['lowercase_text'].str.replace('—', ' ')\n",
    "train['lowercase_text']=train['lowercase_text'].str.replace('–', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59756b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[[\"lowercase_text\"]]\n",
    "X[\"lowercase_text\"] = X[\"lowercase_text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', remove_punct))\n",
    "\n",
    "X[\"wo_punct\"] = X[\"lowercase_text\"].apply(lambda text: remove_punctuation(text))\n",
    "X['final_text']=X['wo_punct'].str.lower()\n",
    "#train.drop([\"lowercase_text\"], axis=1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(final_text):\n",
    "    \n",
    "    return \" \".join([word for word in str(final_text).split() if word not in stop_words])\n",
    "\n",
    "X[\"final_text\"] = X[\"final_text\"].apply(lambda text: remove_stopwords(text))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"final_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87480d61",
   "metadata": {},
   "source": [
    "Подсчитываем наиболее часто встречающиеся слова..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "counter = Counter()\n",
    "for text in X[\"final_text\"]:\n",
    "    for word in text.split():\n",
    "        counter[word]+=1\n",
    "print(type(counter))\n",
    "\n",
    "common_words = counter.most_common(10)\n",
    "print(type(common_words), common_words)\n",
    "\n",
    "common_words1 = []\n",
    "freq_words1 = []\n",
    "\n",
    "for word, freq in common_words:\n",
    "    common_words1.append(word)\n",
    "    freq_words1.append(freq)\n",
    "    plt.bar(common_words1, freq_words1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f11953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "X[\"final_text\"] = X[\"final_text\"].apply(lambda text: lemmatizing(text))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = ((X[\"final_text\"]).tolist())\n",
    "X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f0c77170",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    " \n",
    "doc = Doc(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = getLogger(__name__)\n",
    "\n",
    "\n",
    "@register('hybrid_ner_model')\n",
    "class HybridNerModel(LRScheduledTFModel):\n",
    "    \"\"\" This class implements the hybrid NER model published in the paper: http://www.ijmlc.org/show-83-881-1.html\n",
    "\n",
    "    Params:\n",
    "        n_tags: Number of pre-defined tags.\n",
    "        word_emb_path: The path to the pretrained word embedding model.\n",
    "        word_emb_name: The name of pretrained word embedding model.\n",
    "            One of the two values should be set including 'glove', 'baomoi' corresponding to two pre-trained word\n",
    "            embedding models: GloVe (https://www.aclweb.org/anthology/D14-1162/)\n",
    "            and baomoi (https://github.com/sonvx/word2vecVN). Otherwise, the word lookup table will be trained\n",
    "            from scratch.\n",
    "        word_vocab: The word vocabulary class.\n",
    "        word_dim: The dimension of the pretrained word vector.\n",
    "        char_vocab_size: The size of character vocabulary.\n",
    "        pos_vocab_size: The size of POS vocabulary.\n",
    "        chunk_vocab_size: The size of Chunk vocabulary.\n",
    "        char_dim: The dimension of character vector.\n",
    "        elmo_dim: The dimension of ELMo-based word vector\n",
    "        elmo_hub_path: The path to the ELmo tensorhub\n",
    "        pos_dim: The dimension of POS vector.\n",
    "        chunk_dim: The dimension of Chunk vector.\n",
    "        cap_dim: The dimension of capitalization vector.\n",
    "        cap_vocab_size: The size of capitalization vocabulary.\n",
    "        lstm_hidden_size: The number of units in contextualized Bi-LSTM network\n",
    "        drop_out_keep_prob: The probability of keeping hidden state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tags: int,\n",
    "                 word_vocab,\n",
    "                 word_dim: int,\n",
    "                 word_emb_path: str,\n",
    "                 word_emb_name: str = None,\n",
    "                 char_vocab_size: int = None,\n",
    "                 pos_vocab_size: int = None,\n",
    "                 chunk_vocab_size: int = None,\n",
    "                 char_dim: int = None,\n",
    "                 elmo_dim: int = None,\n",
    "                 elmo_hub_path: str = \"https://tfhub.dev/google/elmo/2\",\n",
    "                 pos_dim: int = None,\n",
    "                 chunk_dim: int = None,\n",
    "                 cap_dim: int = None,\n",
    "                 cap_vocab_size: int = 5,\n",
    "                 lstm_hidden_size: int = 256,\n",
    "                 dropout_keep_prob: float = 0.5,\n",
    "                 **kwargs) -> None:\n",
    "\n",
    "        assert n_tags != 0, 'Number of classes equal 0! It seems that vocabularies is not loaded.' \\\n",
    "                            ' Check that all vocabulary files are downloaded!'\n",
    "\n",
    "        if 'learning_rate_drop_div' not in kwargs:\n",
    "            kwargs['learning_rate_drop_div'] = 10.0\n",
    "        if 'learning_rate_drop_patience' not in kwargs:\n",
    "            kwargs['learning_rate_drop_patience'] = 5.0\n",
    "        if 'clip_norm' not in kwargs:\n",
    "            kwargs['clip_norm'] = 5.0\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        word2id = word_vocab.t2i\n",
    "        word_emb_path = str(expand_path(word_emb_path))\n",
    "\n",
    "        self._dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name='dropout')\n",
    "        self.training_ph = tf.placeholder_with_default(False, shape=[], name='is_training')\n",
    "        self._y_ph = tf.placeholder(tf.int32, [None, None], name='y_ph')\n",
    "\n",
    "        self._xs_ph_list = []\n",
    "        self._input_features = []\n",
    "\n",
    "        # use for word contextualized bi-lstm, elmo\n",
    "        self.real_sent_lengths_ph = tf.placeholder(tf.int32, [None], name=\"real_sent_lengths\")\n",
    "        self._xs_ph_list.append(self.real_sent_lengths_ph)\n",
    "\n",
    "        # Word emb\n",
    "        with tf.variable_scope(\"word_emb\"):\n",
    "            word_ids_ph = tf.placeholder(tf.int32, [None, None], name=\"word_ids\")\n",
    "            self._xs_ph_list.append(word_ids_ph)\n",
    "\n",
    "            word_embeddings = self.load_pretrained_word_emb(word_emb_path, word_emb_name, word_dim, word2id)\n",
    "\n",
    "            word_lookup_table = tf.Variable(word_embeddings, dtype=tf.float32, trainable=True, name=\"word_embeddings\")\n",
    "            word_emb = tf.nn.embedding_lookup(word_lookup_table, word_ids_ph, name=\"embedded_word\")\n",
    "            self._input_features.append(word_emb)\n",
    "\n",
    "        # POS feature\n",
    "        if pos_dim is not None:\n",
    "            with tf.variable_scope(\"pos_emb\"):\n",
    "                pos_ph = tf.placeholder(tf.int32, [None, None], name=\"pos_ids\")\n",
    "                self._xs_ph_list.append(pos_ph)\n",
    "\n",
    "                tf_pos_embeddings = tf.get_variable(name=\"pos_embeddings\",\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    shape=[pos_vocab_size, pos_dim],\n",
    "                                                    trainable=True,\n",
    "                                                    initializer=xavier_initializer())\n",
    "\n",
    "                embedded_pos = tf.nn.embedding_lookup(tf_pos_embeddings,\n",
    "                                                      pos_ph,\n",
    "                                                      name=\"embedded_pos\")\n",
    "                self._input_features.append(embedded_pos)\n",
    "\n",
    "        # Chunk feature\n",
    "        if chunk_dim is not None:\n",
    "            with tf.variable_scope(\"chunk_emb\"):\n",
    "                chunk_ph = tf.placeholder(tf.int32, [None, None], name=\"chunk_ids\")\n",
    "                self._xs_ph_list.append(chunk_ph)\n",
    "\n",
    "                tf_chunk_embeddings = tf.get_variable(name=\"chunk_embeddings\",\n",
    "                                                      dtype=tf.float32,\n",
    "                                                      shape=[chunk_vocab_size, chunk_dim],\n",
    "                                                      trainable=True,\n",
    "                                                      initializer=xavier_initializer())\n",
    "\n",
    "                embedded_chunk = tf.nn.embedding_lookup(tf_chunk_embeddings,\n",
    "                                                        chunk_ph,\n",
    "                                                        name=\"embedded_chunk\")\n",
    "                self._input_features.append(embedded_chunk)\n",
    "\n",
    "        # Capitalization feature\n",
    "        if cap_dim is not None:\n",
    "            with tf.variable_scope(\"cap_emb\"):\n",
    "                cap_ph = tf.placeholder(tf.int32, [None, None], name=\"cap_ids\")\n",
    "                self._xs_ph_list.append(cap_ph)\n",
    "\n",
    "                tf_cap_embeddings = tf.get_variable(name=\"cap_embeddings\",\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    shape=[cap_vocab_size, cap_dim],\n",
    "                                                    trainable=True,\n",
    "                                                    initializer=xavier_initializer())\n",
    "\n",
    "                embedded_cap = tf.nn.embedding_lookup(tf_cap_embeddings,\n",
    "                                                      cap_ph,\n",
    "                                                      name=\"embedded_cap\")\n",
    "                self._input_features.append(embedded_cap)\n",
    "\n",
    "        # Character feature\n",
    "        if char_dim is not None:\n",
    "            with tf.variable_scope(\"char_emb\"):\n",
    "                char_ids_ph = tf.placeholder(tf.int32, [None, None, None], name=\"char_ids\")\n",
    "                self._xs_ph_list.append(char_ids_ph)\n",
    "\n",
    "                tf_char_embeddings = tf.get_variable(name=\"char_embeddings\",\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     shape=[char_vocab_size, char_dim],\n",
    "                                                     trainable=True,\n",
    "                                                     initializer=xavier_initializer())\n",
    "                embedded_cnn_chars = tf.nn.embedding_lookup(tf_char_embeddings,\n",
    "                                                            char_ids_ph,\n",
    "                                                            name=\"embedded_cnn_chars\")\n",
    "                conv1 = tf.layers.conv2d(inputs=embedded_cnn_chars,\n",
    "                                         filters=128,\n",
    "                                         kernel_size=(1, 3),\n",
    "                                         strides=(1, 1),\n",
    "                                         padding=\"same\",\n",
    "                                         name=\"conv1\",\n",
    "                                         kernel_initializer=xavier_initializer_conv2d())\n",
    "                conv2 = tf.layers.conv2d(inputs=conv1,\n",
    "                                         filters=128,\n",
    "                                         kernel_size=(1, 3),\n",
    "                                         strides=(1, 1),\n",
    "                                         padding=\"same\",\n",
    "                                         name=\"conv2\",\n",
    "                                         kernel_initializer=xavier_initializer_conv2d())\n",
    "                char_cnn = tf.reduce_max(conv2, axis=2)\n",
    "\n",
    "                self._input_features.append(char_cnn)\n",
    "\n",
    "        # ELMo\n",
    "        if elmo_dim is not None:\n",
    "            with tf.variable_scope(\"elmo_emb\"):\n",
    "                padded_x_tokens_ph = tf.placeholder(tf.string, [None, None], name=\"padded_x_tokens\")\n",
    "                self._xs_ph_list.append(padded_x_tokens_ph)\n",
    "\n",
    "                elmo = hub.Module(elmo_hub_path, trainable=True)\n",
    "                emb = elmo(inputs={\"tokens\": padded_x_tokens_ph, \"sequence_len\": self.real_sent_lengths_ph},\n",
    "                           signature=\"tokens\", as_dict=True)[\"elmo\"]\n",
    "                elmo_emb = tf.layers.dense(emb, elmo_dim, activation=None)\n",
    "                self._input_features.append(elmo_emb)\n",
    "\n",
    "        features = tf.nn.dropout(tf.concat(self._input_features, axis=2), self._dropout_ph)\n",
    "\n",
    "        with tf.variable_scope(\"bi_lstm_words\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(lstm_hidden_size)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(lstm_hidden_size)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, features,\n",
    "                                                                        sequence_length=self.real_sent_lengths_ph,\n",
    "                                                                        dtype=tf.float32)\n",
    "            self.output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "            ntime_steps = tf.shape(self.output)[1]\n",
    "            self.output = tf.reshape(self.output, [-1, 2 * lstm_hidden_size])\n",
    "            layer1 = tf.nn.dropout(tf.layers.dense(inputs=self.output, units=lstm_hidden_size, activation=None,\n",
    "                                                   kernel_initializer=xavier_initializer()), self._dropout_ph)\n",
    "            pred = tf.layers.dense(inputs=layer1, units=n_tags, activation=None,\n",
    "                                   kernel_initializer=xavier_initializer())\n",
    "            self.logits = tf.reshape(pred, [-1, ntime_steps, n_tags])\n",
    "\n",
    "            log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(self.logits,\n",
    "                                                                                       self._y_ph,\n",
    "                                                                                       self.real_sent_lengths_ph)\n",
    "        # loss and opt\n",
    "        with tf.variable_scope(\"loss_and_opt\"):\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "            self.train_op = self.get_train_op(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.load()\n",
    "\n",
    "    def predict(self, xs):\n",
    "        feed_dict = self._fill_feed_dict(xs)\n",
    "        logits, trans_params, sent_lengths = self.sess.run([self.logits,\n",
    "                                                            self.transition_params,\n",
    "                                                            self.real_sent_lengths_ph],\n",
    "                                                           feed_dict=feed_dict)\n",
    "        # iterate over the sentences because no batching in viterbi_decode\n",
    "        y_pred = []\n",
    "        for logit, sequence_length in zip(logits, sent_lengths):\n",
    "            logit = logit[:int(sequence_length)]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "            y_pred += [viterbi_seq]\n",
    "        return y_pred\n",
    "\n",
    "    def _fill_feed_dict(self, xs, y=None, train=False):\n",
    "        assert len(xs) == len(self._xs_ph_list)\n",
    "        xs = list(xs)\n",
    "        for x in xs[1:]:\n",
    "            x = np.array(x)\n",
    "        feed_dict = {ph: x for ph, x in zip(self._xs_ph_list, xs)}\n",
    "        if y is not None:\n",
    "            feed_dict[self._y_ph] = y\n",
    "        feed_dict[self.training_ph] = train\n",
    "        if not train:\n",
    "            feed_dict[self._dropout_ph] = 1.0\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if len(args[0]) == 0 or (args[0] == [0]):\n",
    "            return []\n",
    "        return self.predict(args)\n",
    "\n",
    "    def train_on_batch(self, *args):\n",
    "        *xs, y = args\n",
    "        feed_dict = self._fill_feed_dict(xs, y, train=True)\n",
    "        _, loss_value = self.sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return {'loss': loss_value,\n",
    "                'learning_rate': self.get_learning_rate(),\n",
    "                'momentum': self.get_momentum()}\n",
    "\n",
    "    def load_pretrained_word_emb(self, model_path, model_name, word_dim, word2id=None, vocab_size=None):\n",
    "        loaded_words = 0\n",
    "        if word2id is not None:\n",
    "            vocab_size = len(word2id)\n",
    "        word_embeddings = np.zeros(shape=(vocab_size, word_dim))\n",
    "\n",
    "        if model_name == \"glove\":\n",
    "            model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "            for word in word2id:\n",
    "                if word in model:\n",
    "                    word_embeddings[word2id[word]] = model[word]\n",
    "                    loaded_words += 1\n",
    "        elif model_name == \"baomoi\":\n",
    "            model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')\n",
    "            for word in word2id:\n",
    "                if len(word) == 1:\n",
    "                    if word[0] in string.punctuation:\n",
    "                        word_embeddings[word2id[word]] = model[\"<punct>\"]\n",
    "                        loaded_words += 1\n",
    "                elif word.isdigit():\n",
    "                    word_embeddings[word2id[word]] = model[\"<number>\"]\n",
    "                    loaded_words += 1\n",
    "                elif word in model.vocab:\n",
    "                    word_embeddings[word2id[word]] = model[word]\n",
    "                    loaded_words += 1\n",
    "        elif model_name == \"fasttext\":\n",
    "            ft_model = FastText.load_fasttext_format(model_path)\n",
    "            for word in word2id:\n",
    "                if word in ft_model.wv.vocab:\n",
    "                    word_embeddings[word2id[word]] = ft_model.wv[word]\n",
    "                    loaded_words += 1\n",
    "        elif model_name is not None:\n",
    "            raise RuntimeError(f'got an unexpected value for model_name: `{model_name}`')\n",
    "\n",
    "        log.info(f\"{loaded_words}/{vocab_size} words were loaded from {model_path}.\")\n",
    "        return word_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
